<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Text analysis   ðŸ“ƒ</title>
    <meta charset="utf-8" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/font-awesome/css/all.css" rel="stylesheet" />
    <link href="libs/font-awesome/css/v4-shims.css" rel="stylesheet" />
    <link href="libs/panelset/panelset.css" rel="stylesheet" />
    <script src="libs/panelset/panelset.js"></script>
    <script src="libs/htmlwidgets/htmlwidgets.js"></script>
    <link href="libs/datatables-css/datatables-crosstalk.css" rel="stylesheet" />
    <script src="libs/datatables-binding/datatables.js"></script>
    <script src="libs/jquery/jquery-3.6.0.min.js"></script>
    <link href="libs/dt-core/css/jquery.dataTables.min.css" rel="stylesheet" />
    <link href="libs/dt-core/css/jquery.dataTables.extra.css" rel="stylesheet" />
    <script src="libs/dt-core/js/jquery.dataTables.min.js"></script>
    <link href="libs/crosstalk/css/crosstalk.min.css" rel="stylesheet" />
    <script src="libs/crosstalk/js/crosstalk.min.js"></script>
    <link rel="stylesheet" href="../slides.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Text analysis <br> ðŸ“ƒ
### 

---





layout: true
  
&lt;div class="my-footer"&gt;
&lt;span&gt;
&lt;a href="https://datasciencebox.org" target="_blank"&gt;datasciencebox.org&lt;/a&gt;
&lt;/span&gt;
&lt;/div&gt; 

---



class: middle

# Tidytext analysis

---

## Packages

In addition to `tidyverse` we will be using four other packages today


```r
library(tidytext)
library(readxl)
library(wordcloud)
library(DT)
```

---

## Tidytext

- Using tidy data principles can make many text mining tasks easier, more effective, and consistent with tools already in wide use.
- Learn more at https://www.tidytextmining.com/.

---

## What is tidy text?


```r
text &lt;- c("Take me out tonight",
          "Where there's music and there's people",
          "And they're young and alive",
          "Driving in your car",
          "I never never want to go home",
          "Because I haven't got one",
          "Anymore")

text
```

```
## [1] "Take me out tonight"                   
## [2] "Where there's music and there's people"
## [3] "And they're young and alive"           
## [4] "Driving in your car"                   
## [5] "I never never want to go home"         
## [6] "Because I haven't got one"             
## [7] "Anymore"
```

---

## What is tidy text?


```r
text_df &lt;- tibble(line = 1:7, text = text)

text_df
```

```
## # A tibble: 7 Ã— 2
##    line text                                  
##   &lt;int&gt; &lt;chr&gt;                                 
## 1     1 Take me out tonight                   
## 2     2 Where there's music and there's people
## 3     3 And they're young and alive           
## 4     4 Driving in your car                   
## 5     5 I never never want to go home         
## 6     6 Because I haven't got one             
## # â€¦ with 1 more row
```

---

## What is tidy text?


```r
text_df %&gt;%
  unnest_tokens(word, text)
```

```
## # A tibble: 32 Ã— 2
##    line word   
##   &lt;int&gt; &lt;chr&gt;  
## 1     1 take   
## 2     1 me     
## 3     1 out    
## 4     1 tonight
## 5     2 where  
## 6     2 there's
## # â€¦ with 26 more rows
```

---

class: middle

# What are you listening to?

---

## From the "Getting to know you" survey

&gt; "What are your 3 - 5 most favorite songs right now?"

.midi[

```r
listening &lt;- read_csv("data/listening.csv")
listening
```

```
## # A tibble: 104 Ã— 1
##   songs                                                          
##   &lt;chr&gt;                                                          
## 1 Gamma Knife - King Gizzard and the Lizard Wizard; Self Immolatâ€¦
## 2 I dont listen to much music                                    
## 3 Mess by Ed Sheeran, Take me back to london by Ed Sheeran and Sâ€¦
## 4 Hate Me (Sometimes) - Stand Atlantic; Edge of Seventeen - Stevâ€¦
## 5 whistle, gogobebe, sassy me                                    
## 6 Shofukan, Think twice, Padiddle                                
## # â€¦ with 98 more rows
```
]

---

## Looking for commonalities

.midi[

```r
listening %&gt;%
  unnest_tokens(word, songs) %&gt;%
  count(word, sort = TRUE)
```

```
## # A tibble: 786 Ã— 2
##   word      n
##   &lt;chr&gt; &lt;int&gt;
## 1 the      56
## 2 by       23
## 3 to       20
## 4 and      19
## 5 i        19
## 6 you      15
## # â€¦ with 780 more rows
```
]

---

## Stop words

- In computing, stop words are words which are filtered out before or after processing of natural language data (text).
- They usually refer to the most common words in a language, but there is not a single list of stop words used by all natural language processing tools.

---

## English stop words


```r
get_stopwords()
```

```
## # A tibble: 175 Ã— 2
##   word   lexicon 
##   &lt;chr&gt;  &lt;chr&gt;   
## 1 i      snowball
## 2 me     snowball
## 3 my     snowball
## 4 myself snowball
## 5 we     snowball
## 6 our    snowball
## # â€¦ with 169 more rows
```

---

## Spanish stop words


```r
get_stopwords(language = "es")
```

```
## # A tibble: 308 Ã— 2
##   word  lexicon 
##   &lt;chr&gt; &lt;chr&gt;   
## 1 de    snowball
## 2 la    snowball
## 3 que   snowball
## 4 el    snowball
## 5 en    snowball
## 6 y     snowball
## # â€¦ with 302 more rows
```

---

## Various lexicons

See `?get_stopwords` for more info.

.midi[

```r
get_stopwords(source = "smart")
```

```
## # A tibble: 571 Ã— 2
##   word      lexicon
##   &lt;chr&gt;     &lt;chr&gt;  
## 1 a         smart  
## 2 a's       smart  
## 3 able      smart  
## 4 about     smart  
## 5 above     smart  
## 6 according smart  
## # â€¦ with 565 more rows
```
]

---

## Back to: Looking for commonalities

.small[

```r
listening %&gt;%
  unnest_tokens(word, songs) %&gt;%
* anti_join(stop_words) %&gt;%
* filter(!(word %in% c("1", "2", "3", "4", "5"))) %&gt;%
  count(word, sort = TRUE)
```

```
## # A tibble: 640 Ã— 2
##   word        n
##   &lt;chr&gt;   &lt;int&gt;
## 1 ed          7
## 2 queen       7
## 3 sheeran     7
## 4 love        6
## 5 bad         5
## 6 time        5
## # â€¦ with 634 more rows
```
]

---

## Top 20 common words in songs

.pull-left[
.small[

```r
top20_songs &lt;- listening %&gt;%
  unnest_tokens(word, songs) %&gt;%
  anti_join(stop_words) %&gt;%
  filter(
    !(word %in% c("1", "2", "3", "4", "5"))
    ) %&gt;%
  count(word) %&gt;%
  top_n(20)
```
]
]
.pull-right[
.midi[

```r
top20_songs %&gt;%
  arrange(desc(n))
```

```
## # A tibble: 41 Ã— 2
##   word        n
##   &lt;chr&gt;   &lt;int&gt;
## 1 ed          7
## 2 queen       7
## 3 sheeran     7
## 4 love        6
## 5 bad         5
## 6 time        5
## # â€¦ with 35 more rows
```
]
]
---

## Visualizing commonalities: bar chart

.midi[
&lt;img src="u2-d11-text-analysis_files/figure-html/unnamed-chunk-14-1.png" width="60%" style="display: block; margin: auto;" /&gt;
]

---

... the code


```r
ggplot(top20_songs, aes(x = fct_reorder(word, n), y = n)) +
  geom_col() +
  labs(x = "Common words", y = "Count") +
  coord_flip()
```


---

## Visualizing commonalities: wordcloud

&lt;img src="u2-d11-text-analysis_files/figure-html/unnamed-chunk-16-1.png" width="80%" style="display: block; margin: auto;" /&gt;

---

... and the code


```r
set.seed(1234)
wordcloud(words = top20_songs$word, 
          freq = top20_songs$n, 
          colors = brewer.pal(5,"Blues"),
          random.order = FALSE)
```

---

## Ok, so people like Ed Sheeran!


```r
str_subset(listening$songs, "Sheeran")
```

```
## [1] "Mess by Ed Sheeran, Take me back to london by Ed Sheeran and Sounds of the Skeng by Stormzy"                
## [2] "Ed Sheeran- I don't care, beautiful people, don't"                                                          
## [3] "Truth Hurts by Lizzo , Wetsuit by The Vaccines , Beautiful People by Ed Sheeran"                            
## [4] "Sounds of the Skeng - Stormzy, Venom - Eminem, Take me back to london - Ed Sheeran, I see fire - Ed Sheeran"
```

---

## But I had to ask...

--

What is 1975?

--


```r
str_subset(listening$songs, "1975")
```

```
## [1] "Hate Me (Sometimes) - Stand Atlantic; Edge of Seventeen - Stevie Nicks; It's Not Living (If It's Not With You) - The 1975; People - The 1975; Hypersonic Missiles - Sam Fender"
## [2] "Chocolate by the 1975, sanctuary by Joji, A young understating by Sundara Karma"                                                                                               
## [3] "Lauv - I'm lonely, kwassa - good life, the 1975 - sincerity is scary"
```

---

class: middle

# Analyzing lyrics of one artist

---

## Let's get more data

---

## Ed's albums


```r
if (!file.exists("data/Lyrics.xlsx")) {
  download.file("https://raw.githubusercontent.com/chitinglow/ed_sheeran_song_analysis/c3e0f8d1643dac23187588de74af57ea380b0657/Lyrics.xlsx",
                "data/Lyrics.xlsx")
}
sheeran &lt;- read_xlsx('data/Lyrics.xlsx') %&gt;% 
  rename(album = Album, track_title = Song_name, lyric = Lyrics)
```

---

## Songs in the four albums

.small[
<div id="htmlwidget-f9c65f1018a28164aeae" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-f9c65f1018a28164aeae">{"x":{"filter":"none","vertical":false,"data":[["1","2","3","4","5","6","7","8","9","10","11","12","13","14","15","16","17","18","19","20","21","22","23","24","25","26","27","28","29","30","31","32","33","34","35","36","37","38","39","40","41","42","43","44","45","46","47","48","49","50","51","52","53","54","55","56","57","58","59","60","61","62","63","64","65","66","67","68","69","70","71","72","73","74","75","76","77","78","79","80","81","82","83","84","85","86","87","88","89","90","91","92","93","94","95","96","97","98","99","100","101","102","103","104","105","106","107","108"],["EP Orange Room","EP Orange Room","EP Orange Room","EP Orange Room","EP Orange Room","Ed Sheeran","Ed Sheeran","Ed Sheeran","Ed Sheeran","Ed Sheeran","Ed Sheeran","Ed Sheeran","Ed Sheeran","Ed Sheeran","Ed Sheeran","Ed Sheeran","Ed Sheeran","Ed Sheeran","Want Some?","Want Some?","Want Some?","Want Some?","Want Some?","Want Some?","Want Some?","Want Some?","Want Some?","Want Some?","Want Some?","Want Some?","You Need Me","You Need Me","You Need Me","You Need Me","You Need Me","Loose Change","Loose Change","Loose Change","Loose Change","Loose Change","Loose Change","Songs I Wrote With Amy","Songs I Wrote With Amy","Songs I Wrote With Amy","Songs I Wrote With Amy","Songs I Wrote With Amy","No. 5 Collaborations Project","No. 5 Collaborations Project","No. 5 Collaborations Project","No. 5 Collaborations Project","No. 5 Collaborations Project","No. 5 Collaborations Project","No. 5 Collaborations Project","No. 5 Collaborations Project","No. 5 Collaborations Project","No. 5 Collaborations Project","No. 5 Collaborations Project","No. 5 Collaborations Project","No. 5 Collaborations Project","No. 5 Collaborations Project","No. 5 Collaborations Project","No. 5 Collaborations Project","No. 5 Collaborations Project","No. 5 Collaborations Project","No. 5 Collaborations Project","No. 5 Collaborations Project","No. 5 Collaborations Project","The Slumdon Bridge","The Slumdon Bridge","The Slumdon Bridge","The Slumdon Bridge","X","X","X","X","X","X","X","X","X","X","X","X","X","X","X","X","X","X","X","X","X","Divided","Divided","Divided","Divided","Divided","Divided","Divided","Divided","Divided","Divided","Divided","Divided","Divided","Divided","Divided","Divided"],["I Love you","Addicted","Typical Average","Misery","Moody Ballad Of Ed","Open your ears","Beyond The Pale","In Memory","Quiet Ballad Of Ed","Insomniac's Lullaby","No Luck","Stevensong","Billy Ruskin","Spark","Pause","The sea","Way Home","Bonus Track","You Break Me","I'm Glad I'm Not You","You Need To Cut Your Hair","Sara","You Break Me 2","I Can't Spell","The West Coast Of Clare","Two Blokes And A Double Bass","Postcards","Smile","Yellow Pages","Move On","You Need Me, I Don't Need You","So","Be Like You","The City","Sunburn","The A Team","Homeless","Little Bird","Sofa","One Night","Firefly","Cold Coffee","Fall","Fire Alarms","She","Where We Land","Lately","You","Family","Radio","Little Lady","Drown Me Out","Nightmares","Goodbye To You","Drunk","Uni","Grade 8","Wake Me Up","Small Bump","This","The City","Lego House","Kiss Me","Give Me Love","The Parting Glass","Autumn Leaves","Gold Rush","London Bridge","You Don't Know","Faces","Tones","One","Nina","I'm Mess","Sing","Don't","Photograph","Bloodstream","Tenerife Sea","Runaway","The Man","Thinking Out Loud","Afire Love","Take It Back","Shirtsleeves","Even My Dad Does Sometimes","I See Fire","All Of The Stars","English Rose","Touch and Go","New York","Make It Rain","Castle Of Hill","Dive","Shape Of You","Perfect","Galway Girl","Happier","New Man","Heart Don't Break Here","What Do I Know","How Would You Feel","Supermarket Flowers","Barcelona","Bibia be ye ye","Nancy Mulligan","Save Myself","Eraser"]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>album<\/th>\n      <th>track_title<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"dom":"p","columnDefs":[{"orderable":false,"targets":0}],"order":[],"autoWidth":false,"orderClasses":false}},"evals":[],"jsHooks":[]}</script>
]

---

## How long are Ed Sheeran's songs?

Length measured by number of lines


```r
sheeran %&gt;%
  count(track_title, sort = TRUE)
```

```
## # A tibble: 107 Ã— 2
##   track_title          n
##   &lt;chr&gt;            &lt;int&gt;
## 1 The City             2
## 2 Addicted             1
## 3 Afire Love           1
## 4 All Of The Stars     1
## 5 Autumn Leaves        1
## 6 Barcelona            1
## # â€¦ with 101 more rows
```

---

## Tidy up your lyrics!


```r
sheeran_lyrics &lt;- sheeran %&gt;%
  unnest_tokens(word, lyric)

sheeran_lyrics
```

```
## # A tibble: 38,576 Ã— 4
##   album          track_title  Year word    
##   &lt;chr&gt;          &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;   
## 1 EP Orange Room I Love you   2005 i'm     
## 2 EP Orange Room I Love you   2005 standing
## 3 EP Orange Room I Love you   2005 on      
## 4 EP Orange Room I Love you   2005 a       
## 5 EP Orange Room I Love you   2005 mountain
## 6 EP Orange Room I Love you   2005 waiting 
## # â€¦ with 38,570 more rows
```

---

## What are the most common words?


```r
sheeran_lyrics %&gt;%
  count(word, sort = TRUE)
```

```
## # A tibble: 3,493 Ã— 2
##   word      n
##   &lt;chr&gt; &lt;int&gt;
## 1 i      1545
## 2 the    1403
## 3 you    1333
## 4 and    1150
## 5 my     1007
## 6 to      890
## # â€¦ with 3,487 more rows
```

---

## What a romantic!

.midi[

```r
sheeran_lyrics %&gt;%
  anti_join(stop_words) %&gt;%
  count(word, sort = TRUE)
```

```
## # A tibble: 3,034 Ã— 2
##   word      n
##   &lt;chr&gt; &lt;int&gt;
## 1 love    353
## 2 home    112
## 3 feel    109
## 4 time    105
## 5 wanna   105
## 6 eyes     89
## # â€¦ with 3,028 more rows
```
]

---

&lt;img src="u2-d11-text-analysis_files/figure-html/unnamed-chunk-26-1.png" width="80%" style="display: block; margin: auto;" /&gt;

---

... and the code


```r
sheeran_lyrics %&gt;%
  anti_join(stop_words) %&gt;%
  count(word)%&gt;%
  top_n(20) %&gt;%
  ggplot(aes(fct_reorder(word, n), n)) +
    geom_col() +
    labs(title = "Frequency of Ed Sheeran's lyrics",
         subtitle = "`Love` tops the chart",
         y = "",
         x = "") +
    coord_flip()
```

---

class: middle

# Sentiment analysis

---

## Sentiment analysis

- One way to analyze the sentiment of a text is to consider the text as a combination of its individual words 
- and the sentiment content of the whole text as the sum of the sentiment content of the individual words

---

## Sentiment lexicons

.pull-left[

```r
get_sentiments("afinn")
```

```
## # A tibble: 2,477 Ã— 2
##   word       value
##   &lt;chr&gt;      &lt;dbl&gt;
## 1 abandon       -2
## 2 abandoned     -2
## 3 abandons      -2
## 4 abducted      -2
## 5 abduction     -2
## 6 abductions    -2
## # â€¦ with 2,471 more rows
```
]
.pull-right[

```r
get_sentiments("bing") 
```

```
## # A tibble: 6,786 Ã— 2
##   word       sentiment
##   &lt;chr&gt;      &lt;chr&gt;    
## 1 2-faces    negative 
## 2 abnormal   negative 
## 3 abolish    negative 
## 4 abominable negative 
## 5 abominably negative 
## 6 abominate  negative 
## # â€¦ with 6,780 more rows
```
]

---

## Sentiment lexicons

.pull-left[

```r
get_sentiments("nrc")
```

```
## # A tibble: 13,875 Ã— 2
##   word      sentiment
##   &lt;chr&gt;     &lt;chr&gt;    
## 1 abacus    trust    
## 2 abandon   fear     
## 3 abandon   negative 
## 4 abandon   sadness  
## 5 abandoned anger    
## 6 abandoned fear     
## # â€¦ with 13,869 more rows
```
]
.pull-right[

```r
get_sentiments("loughran") 
```

```
## # A tibble: 4,150 Ã— 2
##   word         sentiment
##   &lt;chr&gt;        &lt;chr&gt;    
## 1 abandon      negative 
## 2 abandoned    negative 
## 3 abandoning   negative 
## 4 abandonment  negative 
## 5 abandonments negative 
## 6 abandons     negative 
## # â€¦ with 4,144 more rows
```
]

---

class: middle

## Categorizing sentiments

---

## Sentiments in Sheeran's lyrics

.midi[

```r
sheeran_lyrics %&gt;%
  inner_join(get_sentiments("bing")) %&gt;%
  count(sentiment, word, sort = TRUE)
```

```
## # A tibble: 523 Ã— 3
##   sentiment word      n
##   &lt;chr&gt;     &lt;chr&gt; &lt;int&gt;
## 1 positive  love    353
## 2 positive  like    231
## 3 positive  right    55
## 4 negative  fall     54
## 5 negative  cold     45
## 6 positive  well     40
## # â€¦ with 517 more rows
```
]

---

class: middle

**Goal:** Find the top 10 most common words with positive and negative sentiments.

---

### Step 1: Top 10 words for each sentiment

.midi[

```r
sheeran_lyrics %&gt;%
  inner_join(get_sentiments("bing")) %&gt;%
  count(sentiment, word) %&gt;%
  group_by(sentiment) %&gt;%
  top_n(10) 
```

```
## # A tibble: 20 Ã— 3
## # Groups:   sentiment [2]
##   sentiment word        n
##   &lt;chr&gt;     &lt;chr&gt;   &lt;int&gt;
## 1 negative  break      28
## 2 negative  broken     21
## 3 negative  cold       45
## 4 negative  drunk      27
## 5 negative  fall       54
## 6 negative  falling    29
## # â€¦ with 14 more rows
```
]

---

### Step 2: `ungroup()`

.midi[

```r
sheeran_lyrics %&gt;%
  inner_join(get_sentiments("bing")) %&gt;%
  count(sentiment, word) %&gt;%
  group_by(sentiment) %&gt;%
  top_n(10) %&gt;%
  ungroup()
```

```
## # A tibble: 20 Ã— 3
##   sentiment word        n
##   &lt;chr&gt;     &lt;chr&gt;   &lt;int&gt;
## 1 negative  break      28
## 2 negative  broken     21
## 3 negative  cold       45
## 4 negative  drunk      27
## 5 negative  fall       54
## 6 negative  falling    29
## # â€¦ with 14 more rows
```
]

---

### Step 3: Save the result


```r
sheeran_top10 &lt;- sheeran_lyrics %&gt;%
  inner_join(get_sentiments("bing")) %&gt;%
  count(sentiment, word) %&gt;%
  group_by(sentiment) %&gt;%
  top_n(10) %&gt;%
  ungroup()
```

---

class: middle

**Goal:** Visualize the top 10 most common words with positive and negative sentiments.

---

### Step 1: Create a bar chart

.midi[

```r
sheeran_top10 %&gt;%
  ggplot(aes(x = word, y = n, fill = sentiment)) +
  geom_col()
```

&lt;img src="u2-d11-text-analysis_files/figure-html/unnamed-chunk-36-1.png" width="80%" style="display: block; margin: auto;" /&gt;
]

---

### Step 2: Order bars by frequency

.midi[

```r
sheeran_top10 %&gt;%
  ggplot(aes(x = fct_reorder(word, n), y = n, fill = sentiment)) +
  geom_col()
```

&lt;img src="u2-d11-text-analysis_files/figure-html/unnamed-chunk-37-1.png" width="80%" style="display: block; margin: auto;" /&gt;
]

---

### Step 3: Facet by sentiment

.midi[

```r
sheeran_top10 %&gt;%
  ggplot(aes(x = fct_reorder(word, n), y = n, fill = sentiment)) +
  geom_col() +
  facet_wrap(~ sentiment)
```

&lt;img src="u2-d11-text-analysis_files/figure-html/unnamed-chunk-38-1.png" width="80%" style="display: block; margin: auto;" /&gt;
]

---

### Step 4: Free the scales!

.midi[

```r
sheeran_top10 %&gt;%
  ggplot(aes(x = fct_reorder(word, n), y = n, fill = sentiment)) +
  geom_col() +
  facet_wrap(~ sentiment, scales = "free")
```

&lt;img src="u2-d11-text-analysis_files/figure-html/unnamed-chunk-39-1.png" width="80%" style="display: block; margin: auto;" /&gt;
]

---

### Step 4: Flip the coordinates

.midi[

```r
sheeran_top10 %&gt;%
  ggplot(aes(x = fct_reorder(word, n), y = n, fill = sentiment)) +
  geom_col() +
  facet_wrap(~ sentiment, scales = "free") +
  coord_flip()
```

&lt;img src="u2-d11-text-analysis_files/figure-html/unnamed-chunk-40-1.png" width="80%" style="display: block; margin: auto;" /&gt;
]

---

### Step 5: Clean up labels

.small[

```r
sheeran_top10 %&gt;%
  ggplot(aes(x = fct_reorder(word, n), y = n, fill = sentiment)) +
  geom_col() +
  facet_wrap(~ sentiment, scales = "free") +
  coord_flip() +
  labs(title = "Sentiments in Ed Sheeran's lyrics", x = "", y = "")
```

&lt;img src="u2-d11-text-analysis_files/figure-html/unnamed-chunk-41-1.png" width="80%" style="display: block; margin: auto;" /&gt;
]

---

### Step 6: Remove redundant info

.small[

```r
sheeran_top10 %&gt;%
  ggplot(aes(x = fct_reorder(word, n), y = n, fill = sentiment)) +
  geom_col() +
  facet_wrap(~ sentiment, scales = "free") +
  coord_flip() +
  labs(title = "Sentiments in Ed Sheeran's lyrics", x = "", y = "") +
  guides(fill = FALSE) 
```

```
## Warning: `guides(&lt;scale&gt; = FALSE)` is deprecated. Please use
## `guides(&lt;scale&gt; = "none")` instead.
```

&lt;img src="u2-d11-text-analysis_files/figure-html/unnamed-chunk-42-1.png" width="80%" style="display: block; margin: auto;" /&gt;
]

---

class: middle

## Scoring sentiments

---

## Assign a sentiment score

.small[

```r
sheeran_lyrics %&gt;%
  anti_join(stop_words) %&gt;%
  left_join(get_sentiments("afinn")) 
```

```
## # A tibble: 11,861 Ã— 5
##   album          track_title  Year word     value
##   &lt;chr&gt;          &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;
## 1 EP Orange Room I Love you   2005 standing    NA
## 2 EP Orange Room I Love you   2005 mountain    NA
## 3 EP Orange Room I Love you   2005 waiting     NA
## 4 EP Orange Room I Love you   2005 sitting     NA
## 5 EP Orange Room I Love you   2005 counting    NA
## 6 EP Orange Room I Love you   2005 days        NA
## # â€¦ with 11,855 more rows
```
]

---


```r
sheeran_lyrics %&gt;%
  anti_join(stop_words) %&gt;%
  left_join(get_sentiments("afinn")) %&gt;%
  filter(!is.na(value)) %&gt;%
  group_by(album) %&gt;%
  summarise(total_sentiment = sum(value)) %&gt;%
  arrange(total_sentiment)
```

```
## # A tibble: 10 Ã— 2
##   album                  total_sentiment
##   &lt;chr&gt;                            &lt;dbl&gt;
## 1 The Slumdon Bridge                -136
## 2 Want Some?                        -106
## 3 Ed Sheeran                        -101
## 4 You Need Me                        -28
## 5 Loose Change                       -15
## 6 Songs I Wrote With Amy               7
## # â€¦ with 4 more rows
```

---

&lt;img src="u2-d11-text-analysis_files/figure-html/unnamed-chunk-45-1.png" width="60%" style="display: block; margin: auto;" /&gt;

---

## Acknowledgements

- Julia Silge: https://github.com/juliasilge/tidytext-tutorial
- Julia Silge and David Robinson: https://www.tidytextmining.com/
- Josiah Parry: https://github.com/JosiahParry/genius
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9",
"highlightLines": true,
"highlightStyle": "solarized-light",
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
